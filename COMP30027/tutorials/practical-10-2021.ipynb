{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Week 10 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.(a) - Viterbi algorithm\n",
    "\n",
    "Here we'll show how the Viterbi algorithm works for HMMs, assuming we have a trained model to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the HMM transition matrix, output probability matrix and initial probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden states / tags / labels\n",
    "tagNames = 'NNP', 'MD', 'VB', 'JJ', 'NN', 'RB', 'DT'\n",
    "tags = 0, 1, 2, 3, 4, 5, 6\n",
    "tag_dict = {0: 'NNP',\n",
    "           1: 'MD',\n",
    "           2: 'VB',\n",
    "           3: 'JJ',\n",
    "           4: 'NN',\n",
    "           5: 'RB',\n",
    "           6: 'DT'}\n",
    "#observations\n",
    "wordNames = 'Janet', 'will', 'back', 'the', 'bill'\n",
    "words = 0, 1, 2, 3, 4\n",
    "\n",
    "#transition probs\n",
    "A = np.array([\n",
    "    [0.3777, 0.0110, 0.0009, 0.0084, 0.0584, 0.0090, 0.0025],\n",
    "    [0.0008, 0.0002, 0.7968, 0.0005, 0.0008, 0.1698, 0.0041],\n",
    "    [0.0322, 0.0005, 0.0050, 0.0837, 0.0615, 0.0514, 0.2231],\n",
    "    [0.0366, 0.0004, 0.0001, 0.0733, 0.4509, 0.0036, 0.0036],\n",
    "    [0.0096, 0.0176, 0.0014, 0.0086, 0.1216, 0.0177, 0.0068],\n",
    "    [0.0068, 0.0102, 0.1011, 0.1012, 0.0120, 0.0728, 0.0479],\n",
    "    [0.1147, 0.0021, 0.0002, 0.2157, 0.4744, 0.0102, 0.0017]\n",
    "    ])\n",
    "\n",
    "#initial state probabilities\n",
    "pi = np.array([0.2767, 0.0006, 0.0031, 0.0453, 0.0449, 0.0510, 0.2026])\n",
    "\n",
    "#output probability probs\n",
    "B = np.array([\n",
    "    [0.000032, 0, 0, 0.000048, 0],\n",
    "    [0, 0.308431, 0, 0, 0],\n",
    "    [0, 0.000028, 0.000672, 0, 0.000028],\n",
    "    [0, 0, 0.000340, 0.000097, 0],\n",
    "    [0, 0.000200, 0.000223, 0.000006, 0.002337],\n",
    "    [0, 0, 0.010446, 0, 0],\n",
    "    [0, 0, 0, 0.506099, 0]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll code the Viterbi algorithm. It keeps a store of two components, the best scores to reach a state at a given time, and the last step of the path to get there. Scores delta are initialised to -inf to denote that we haven't set them yet. \n",
    "\n",
    "Initialisation:\n",
    "\n",
    "* initialise delta \n",
    "* initialise backpointer (psi) parameters\n",
    "\n",
    "$$ \\delta_t(i) = -\\inf$$\n",
    "$$\\textrm{backpointers} \\quad \\psi_1(i) = 0 \\quad \\textrm{for all timesteps and states}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.zeros((len(tags), len(words))) # states x time steps\n",
    "delta[:,:] = float('-inf')\n",
    "backpointers = np.zeros((len(tags), len(words)), 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base case for the recursion sets the starting state probs based on pi and generating the observation. (Note: we also change Numpy precision when printing for better viewing)\n",
    "\n",
    "$$\\delta_1(i) = \\pi_ib_i(o_1),\\,\\, 1\\le i \\le N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base case, time step 0\n",
    "delta[:, 0] = pi * ...  #0:Janet\n",
    "np.set_printoptions(precision=2)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the recursive step, where we maximise over incoming transitions reusing the best incoming score, computed above.\n",
    "\n",
    " $$\\delta_t(j) = \\max_{1\\le i \\le N} \\left(\\delta_{t-1}(i)a_{ij}\\right) b_j(o_t),\\,\\, 2\\le t \\le T,\\,\\, 1\\le j\\le N$$\n",
    " \n",
    " $$ \\psi_t(j) = \\textrm{argmax}_{1\\le i \\le N} \\left(\\delta_{t-1}(i)a_{ij}\\right),\\,\\, 2\\le t \\le T,\\,\\, 1\\le j\\le N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time step 1\n",
    "for t1 in tags:\n",
    "    for t0 in tags:\n",
    "        score = ... * ... * ...  #2: will\n",
    "        if score > ...:\n",
    "            ... = score\n",
    "            backpointers[t1, 1] = ...\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the running maximum for any incoming state (t0) is maintained in delta[1,t1], and the winning state is stored in addition, as a backpointer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with the next observations. (We'd do this as a loop over positions in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time step 2\n",
    "for t2 in tags:\n",
    "    for t1 in tags:\n",
    "        score = ... * ... * ...  #2:back\n",
    "        if score > delta[t2, 2]:\n",
    "            delta[t2, 2] = score\n",
    "            backpointers[t2, 2] = t1\n",
    "print(delta)\n",
    "\n",
    "# time step 3\n",
    "for t3 in tags:\n",
    "    for t2 in tags:\n",
    "        score = ... * ... * ...  #3:the\n",
    "        if score > delta[t3, 3]:\n",
    "            delta[t3, 3] = score\n",
    "            backpointers[t3, 3] = t2\n",
    "print(delta)\n",
    "\n",
    "# time step 4\n",
    "for t4 in tags:\n",
    "    for t3 in tags:\n",
    "        score = ... * ... * ...  #4:bill\n",
    "        if score > delta[t4, 4]:\n",
    "            delta[t4, 4] = score\n",
    "            backpointers[t4, 4] = t3\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read of the best final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = np.argmax(delta[:, 4])\n",
    "print(tag_dict[t4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out the rest of the path which is the best way to reach the final state, t2. We can work this out by taking a step backwards looking at the best incoming edge, i.e., as stored in the backpointers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = ...\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this until we reach the start of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = ...\n",
    "print(tag_dict[t2])\n",
    "t1 = ...\n",
    "print(tag_dict[t1])\n",
    "t0 = ...\n",
    "print(tag_dict[t0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.(b) Formalising things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this all into a function to handle arbitrary length inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(params, words):\n",
    "    pi, A, B = params\n",
    "    N = len(words)\n",
    "    T = pi.shape[0]\n",
    "    \n",
    "    delta = np.zeros((T, N))\n",
    "    delta[:, :] = float('-inf')\n",
    "    backpointers = np.zeros((T, N), 'int')\n",
    "    \n",
    "    # base case\n",
    "    delta[:, 0] = pi * B[:, words[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for w in range(1, N):\n",
    "        for t2 in range(T):\n",
    "            for t1 in range(T):\n",
    "                score = delta[...] * A[...] * B[...]\n",
    "                if score > delta[...]:\n",
    "                    delta[...] = score\n",
    "                    backpointers[...] = ...\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    output = []\n",
    "    output.append(np.argmax(delta[:, N-1]))\n",
    "    for i in range(N-1, 0, -1):\n",
    "        output.append(backpointers[...])\n",
    "    \n",
    "    return list(reversed(output)), np.max(delta[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method on the same input, and a longer input observation sequence. Notice that we are using only 5 words as the vocabulary so we have to restrict tests to sentences containing only these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, score = viterbi((pi, A, B), [0, 1, 2, 3, 4])  #[Janet, will, back, the, bill]\n",
    "print([tag_dict[o] for o in output])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, score = viterbi((pi, A, B), [0, 1, 2, 3, 0, 2, 4])  #[Janet, will, back, the, Janet, back, bill]\n",
    "print([tag_dict[o] for o in output])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - Exhaustive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we've done the above algorithm correctly by implementing exhaustive search, which forms the cross-product of states^M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def exhaustive(params, words):\n",
    "    pi, A, B = params\n",
    "    N = len(words)\n",
    "    T = pi.shape[0]\n",
    "    \n",
    "    # track the running best sequence and its score\n",
    "    best = (None, float('-inf'))\n",
    "    # loop over the cartesian product of |states|^M\n",
    "    num_sequences = 0\n",
    "    for ss in product(range(T), repeat=N):\n",
    "        num_sequences += 1\n",
    "        # score the state sequence\n",
    "        score = pi[...] * B[...]\n",
    "        for i in range(1, N):\n",
    "            score *= A[...] * B[...]\n",
    "        # update the running best\n",
    "        if score > best[1]:\n",
    "            best = (ss, score)\n",
    "    print(f\"num sequences:{num_sequences}\")\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, score = exhaustive((pi, A, B), [0, 1, 2, 3, 4])  #[Janet, will, back, the, bill]\n",
    "print([tag_dict[o] for o in tag_dict])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, score = exhaustive((pi, A, B), [0, 1, 2, 3, 0, 2, 4])  #[Janet, will, back, the, Janet, back, bill]  bill]\n",
    "print([tag_dict[o] for o in tag_dict])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it got the same results as before. Note that the exhaustive method is not practical on anything beyond toy data due to the nasty cartesian product. But it is worth doing to verify the Viterbi code above is getting the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise3- Supervised training, aka \"visible\" Markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the HMM parameters on the Penn Treebank, using the sample from NLTK. Note that this is a small fraction of the treebank, so we shouldn't expect great performance of our method trained only on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = treebank.tagged_sents()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first map words and tags to numbers for compatibility with the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "\n",
    "num_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(...)\n",
    "        ti = tag_numbers.setdefault(...)\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "    \n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's hold out the last few sentences for testing, so that they are unseen during training and give a more reasonable estimate of accuracy on fresh text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = num_corpus[:-10] # reserve the last 10 sentences for testing\n",
    "testing = num_corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute relative frequency estimates based on the observed tag and word counts in the training set. Note that smoothing is important, here we add a small constant to all counts. \n",
    "\n",
    "   $$P(s_j|s_i) = \\frac{\\mathrm{freq}(s_i,s_j)}{\\mathrm{freq}(s_i)} = a_{ij}$$\n",
    "      $$P(o_k|s_i) = \\frac{\\mathrm{freq}(o_k,s_i)}{\\mathrm{freq}(s_i)} = b_{i}(o_k)$$\n",
    "      $$P(q_1 = s_i) = \\frac{\\mathrm{freq}(q_1 = s_i)}{\\sum_j\\mathrm{freq}(q_1 = s_j)} = \\pi_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = len(tag_numbers)\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise with epsilon rather than zero (smoothing)\n",
    "eps = 0.1\n",
    "pi = eps * np.ones(S) # initial\n",
    "A = eps * np.ones((S, S)) #transition\n",
    "B = eps * np.ones((S, V)) #output probability\n",
    "\n",
    "# count\n",
    "for sent in training:\n",
    "    last_tag = None\n",
    "    for word, tag in sent:\n",
    "        B[...] += 1\n",
    "        if last_tag == None:\n",
    "            pi[...] += 1\n",
    "        else:\n",
    "            A[...] += 1\n",
    "        last_tag = tag\n",
    "        \n",
    "# normalise\n",
    "pi /= np.sum(pi)\n",
    "for s in range(S):\n",
    "    B[...] /= np.sum(B[s,:])\n",
    "    A[...] /= np.sum(A[s,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to use our Viterbi method defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, score = viterbi((pi, A, B), list(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED'))\n",
    "for (wi, ti), pi in zip(testing[0], predicted):\n",
    "    print('%20s\\t%5s\\t%5s' % (word_names[wi], tag_names[ti], tag_names[pi]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
