{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 9 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are talking about feature selection and text processing using in `scikit learn`. \n",
    "\n",
    "Remeber you can always use the Scikit Learn API (https://scikit-learn.org/stable/modules/classes.html#) for further exampes and explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "scikit-learn has an in-built text dataset, the “20 newsgroups corpus” https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html), which contains a number of documents classified with a topic, based on the newsgroup in which it was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. (a)\n",
    "Choose a couple of newsgroups that you think it would be interesting to discriminate between, like `rec.autos` and `rec.motorcycles`.\n",
    "\n",
    "We will use `alt.atheism` and `talk.religion.misc` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism','talk.religion.misc']\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "\n",
    "X_train_txt = data_train.data\n",
    "y_train = data_train.target\n",
    "X_test_txt = data_test.data\n",
    "y_test = data_test.target\n",
    "\n",
    "print(dir(data_train))\n",
    "\n",
    "for file,target in zip(data_train.filenames,data_train.target):\n",
    "    print('file:',file, 'label:',target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (b)\n",
    "Examine a couple of documents, by referencing the list ( X_train[0] ). Can you accurately predict the class ( y_train[0] ) based on the text alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInstance Name:\",data_train.filenames[0])\n",
    "print(\"\\nInstance body:\\n\\n\", X_train_txt[0])\n",
    "print('\\n Instance Label:', y_train[0], data_train.target_names[y_train[0]])\n",
    "print('\\n----------------------------------------------------------------------------------')\n",
    "print(\"\\nInstance Name:\",data_train.filenames[3])\n",
    "print(\"\\nInstance body:\\n\\n\", X_train_txt[3])\n",
    "print('\\n Instance Label:', y_train[3], data_train.target_names[y_train[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "The document is currently a string, which scikit-learn can’t use directly. In order to feed predictive models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the `sklearn.feature_extraction.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(a)\n",
    "The class `DictVectorizer` can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\n",
    "\n",
    "`DictVectorizer` implements what is called one-of-K or “one-hot” coding for categorical. Recall that, in this method we replace each categorical attribute having m values with m binary attributes\n",
    "\n",
    "Refresh your knowledge about one-hot encoding and other possible discritisation methods in week 3 and extract the words (or tokens) in the text and count them by using `CountVectorizer` to build a dictionary which associates each word (token) in a text document with its frequency in that document. The list of vocabulary is stored as a dict object in vectoriser.vocabulary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer()\n",
    "\n",
    "X_train = vectoriser.fit_transform(X_train_txt)\n",
    "X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "print(len(vectoriser.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(b)\n",
    "After “vectorizing” the data, what is the shape of X_train and X_test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(c)\n",
    "Are there any documents in X_test whose values are all 0? Why might this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Now that we have \"vectorised\" our text, we want to choose the **best** attributes.\n",
    "\n",
    "Feature Selection in `scikit-learn` can be done using `SelectKBest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(a)\n",
    "Find out what the best 10 features were for your dataset, according to $\\chi^2$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(...,...)\n",
    "\n",
    "X_train_x2 = x2.fit_transform(X_train,y_train)\n",
    "X_test_x2 = x2.transform(X_test)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they correspond to your intuitions? Is there any evidence of the biases inherent in $\\chi^2$? What if you look at deeper than 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(b)\n",
    "Do the same thing for Mutual Information, instead of $\\chi^2$ (note that you want the classification version, not the regression version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi = SelectKBest(...,...)\n",
    "\n",
    "X_train_mi = mi.fit_transform(X_train,y_train)\n",
    "X_test_mi = mi.transform(X_test)\n",
    "\n",
    "print(X_test_mi.shape, X_train_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. \n",
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees.\n",
    "#### Exercise 4.(a) \n",
    "It’s likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer , but before the SelectKBest ) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.(b)  \n",
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.(c) \n",
    "Try some different values for the cut-off for SelectKBest — is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between χ 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          KNeighborsClassifier(n_neighbors=1),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          '1-nearest neighbour',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "k = ...\n",
    "\n",
    "x2 = SelectKBest(...)\n",
    "x2.fit(...,...)\n",
    "X_train_x2 = x2.transform(...)\n",
    "X_test_x2 = x2.transform(...)\n",
    "\n",
    "\n",
    "mi = SelectKBest(...)\n",
    "mi.fit(...,...)\n",
    "X_train_mi = mi.transform(...)\n",
    "X_test_mi = mi.transform(...)\n",
    "\n",
    "\n",
    "Xs = [(X_train, X_test), (X_train_x2, X_test_x2), (X_train_mi, X_test_mi)]\n",
    "\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_test_t.todense(), y_test)\n",
    "        print(X_name, '\\t acc',  acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
