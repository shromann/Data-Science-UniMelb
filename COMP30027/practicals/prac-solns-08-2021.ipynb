{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 8 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we first examine **Logistic Regression** classifier. Then we will use many of the classifier models that we covered so far to build different ensembles and analyse the outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "Let's start with *Logistic Regression*. Use the IRIS dataset (again) and train a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=88)\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train,y_train)\n",
    "print(\"Accuracy:\",lgr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (a)\n",
    "Now using the same split compare the results form Logistic Regression with other classifiers we covered so far. You may use: Zero-R, Gaussian Naive Bayes, Multinomial Naive Bayes,linear SVM, kNN and Decision Tree.\n",
    "\n",
    "Compare their accuracy and the time required for prediction. Analyse the results.\n",
    "\n",
    "Note: Please use the classifiers default hyper parameters (No tunning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R Accuracy: 0.24 Time: 0.0\n",
      "GNB Accuracy: 0.96 Time: 0.001001119613647461\n",
      "MNB Accuracy: 0.58 Time: 0.0\n",
      "LinearSVC Accuracy: 0.9 Time: 0.0\n",
      "Decision Tree Accuracy: 0.9 Time: 0.0\n",
      "KNN Accuracy: 0.92 Time: 0.0020012855529785156\n",
      "Logistic Regression Accuracy: 0.88 Time: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          KNeighborsClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'KNN',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(X_train,y_train)\n",
    "    start = time.time()\n",
    "    acc = model.score(X_test,y_test)\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, \"Accuracy:\",acc, 'Time:', t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `sklearn` is not happy with us when using Linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Because the problem is not fully linear (or linearly separable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.(b)\n",
    "Do the same comparision using the 10-fold Cross-Validation evaluation strategy. Analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R Accuracy: 0.33333333333333337 time: 0.0040035247802734375\n",
      "GNB Accuracy: 0.9533333333333334 time: 0.0070476531982421875\n",
      "MNB Accuracy: 0.9533333333333334 time: 0.006998777389526367\n",
      "LinearSVC Accuracy: 0.9666666666666668 time: 0.06806135177612305\n",
      "Decision Tree Accuracy: 0.96 time: 0.006005525588989258\n",
      "KNN Accuracy: 0.9666666666666668 time: 0.012011051177978516\n",
      "Logistic Regression Accuracy: 0.9533333333333334 time: 0.010009050369262695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, \"Accuracy:\",acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are a few things we can notice here; one is that scikit-learn is not impressed that we are trying to cross-validate with so few instances of each class. This will make stratification impossible, which is undesirable in an evaluation framework.*\n",
    "\n",
    "*Looking at the performance, we can see that except Zero-R most of the classifier have a roughly similar results. What is somewhat surprising here is that the Gaussian NB results are very different beween holdout strategy and cross-validation - perhaps the distributions are not normal because the data has outliers*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Getting to the concept of *stacking*. We want train a meta-classifier (level-1 model) over the outputs of the base classifiers (level-0 model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(a)\n",
    "Using the IRIS dataset, build a stacking of the classifiers:\n",
    "- Zero_R\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Gaussian NB\n",
    "- Multinomial NB\n",
    "- Decsion Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn does support stacking. Check the followig: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "\n",
    "However, to have a better understanding of Stacking, we can implement it by ourselves based on the following steps:\n",
    "- We need to train each of our models (using fit()),\n",
    "- And then classify each training instance (using predict()),\n",
    "- We build up a matrix where the instances are composed of attributes, which correspond to the predictions of each model on this training instance1.\n",
    "- We then train our final learner on this matrix of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You should think about which classifier is most suited to being the final meta–classifier in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Answer*\n",
    "\n",
    "*As mentioned in the lecture slides a simple choice for final meta-classifier of stacking can be logistic regression. We can also try other classifiers like decision tree, or nonlinear SVC as meta-classifier.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [DummyClassifier(strategy='most_frequent'),\n",
    "                LogisticRegression(),\n",
    "                KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB()]\n",
    "titles = ['Zero_R',\n",
    "          'Logistic Regression',\n",
    "          'KNN',\n",
    "          'Gaussian NB',  \n",
    "          'Multinomial NB']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)\n",
    "\n",
    "meta_classifier_dt = DecisionTreeClassifier()\n",
    "stacker_dt = StackingClassifier(classifiers, meta_classifier_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS dataset\n",
      "\n",
      "Zero_R Accuracy: 0.24\n",
      "Logistic Regression Accuracy: 0.88\n",
      "KNN Accuracy: 0.92\n",
      "Gaussian NB Accuracy: 0.96\n",
      "Multinomial NB Accuracy: 0.58\n",
      "\n",
      "Stacker Accuracy (Logistic Regression): 0.96\n",
      "\n",
      "Stacker Accuracy (Decision Tree): 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"IRIS dataset\\n\")\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(title, \"Accuracy:\",clf.score(X_test,y_test))\n",
    "    \n",
    "stacker_lr.fit(X_train, y_train)\n",
    "print('\\nStacker Accuracy (Logistic Regression):', stacker_lr.score(X_test, y_test))\n",
    "\n",
    "stacker_dt.fit(X_train, y_train)\n",
    "print('\\nStacker Accuracy (Decision Tree):', stacker_dt.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Exercise 2.(b)\n",
    "Use the same *stack* to process the `car` dataset use holdout strategy with 30% split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car dataset\n",
      "\n",
      "Zero_R Accuracy: 0.6742556917688266\n",
      "Logistic Regression Accuracy: 0.8458844133099825\n",
      "KNN Accuracy: 0.9001751313485113\n",
      "Gaussian NB Accuracy: 0.8178633975481612\n",
      "Multinomial NB Accuracy: 0.8143607705779334\n",
      "\n",
      "Stacker Accuracy: 0.9457092819614711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def load_data(i_file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(i_file, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            X.append(atts[:-1]) #all atts minus the last one\n",
    "            y.append(atts[-1])\n",
    "    onehot = OneHotEncoder()\n",
    "    X = onehot.fit_transform(X).toarray()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = load_data('car.data')\n",
    "\n",
    "#print('labels:', set(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n",
    "\n",
    "print(\"Car dataset\\n\")\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(title, \"Accuracy:\",clf.score(X_test,y_test))\n",
    "    \n",
    "#meta_classifier_lr = LogisticRegression()\n",
    "#stacker = StackingClassifier(classifiers, meta_classifier_lr)\n",
    "    \n",
    "stacker_dt.fit(X_train, y_train)\n",
    "print('\\nStacker Accuracy:', stacker_dt.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Bagging is often associated with Decision Trees, but in scikit-learn , it can be applied to any learner. \n",
    "\n",
    "If we use bagging with Decision Tree, we will build a number of Decision Trees by re-sampling the data:\n",
    "- For each tree, we randomly select (with repetition) N instances out of the possible N instances, so that we have the same sized data as the deterministic decision tree, but each one is based around a different data set\n",
    "- We then build the tree as usual.\n",
    "- We classify the test instance by **voting** - each tree gets a vote (the class it would predict for the test instance), and the class with the plurality wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(a)\n",
    "Load the `lymphography` dataset and implement bagging of 10 estimator for kNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.40816326530612246\n",
      "KNN Bagging Accuracy: 0.5510204081632653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "X, y = load_data('lymphography.data')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(),n_estimators=10,\\\n",
    "                              max_samples=0.5, max_features=0.5)\n",
    "KNN.fit(X_train,y_train)\n",
    "bagging.fit(X_train,y_train)\n",
    "\n",
    "print(\"KNN:\",KNN.score(X_test,y_test))\n",
    "print(\"KNN Bagging Accuracy:\",bagging.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the documentation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n",
    "\n",
    "Bagging classifier builds N base classifier, each base classifier is trained/fitted on a subset of features/samples. For each base classifier:\n",
    "\n",
    "- Randomly select max_features * X.shape[1] subset of features.\n",
    "- Randomly select max_samples * X.shape[0] subset of samples.\n",
    "- Create a new X_base from the selected features and samples.\n",
    "- Fit the base classifier on X_base and y_base.\n",
    "\n",
    "Then use Voting or averaging to combine the prediction of the base classifier for X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(b)\n",
    "What are the significance of max_samples and max_features , and why might we wish to use values less than 1.0?\n",
    "\n",
    "Build 3 differnt Decision Tree bagging classifiers using differnt combinations of max_samples and max_features. Can you analyse the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: 0.42857142857142855\n",
      "Option 1: bagging Accuracy: 0.46938775510204084\n",
      "Option 2: bagging Accuracy: 0.5102040816326531\n",
      "Option 3: bagging Accuracy: 0.46938775510204084\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "bagging_one = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=1.0, max_features=1.0)\n",
    "bagging_two = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=0.5, max_features=1.0)\n",
    "bagging_three = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=0.5, max_features=0.5)\n",
    "\n",
    "DT.fit(X_train,y_train)\n",
    "bagging_one.fit(X_train,y_train)\n",
    "bagging_two.fit(X_train,y_train)\n",
    "bagging_three.fit(X_train,y_train)\n",
    "\n",
    "print(\"DT:\",DT.score(X_test,y_test))\n",
    "print(\"Option 1: bagging Accuracy:\",bagging_one.score(X_test,y_test))\n",
    "print(\"Option 2: bagging Accuracy:\",bagging_two.score(X_test,y_test))\n",
    "print(\"Option 3: bagging Accuracy:\",bagging_three.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If max_features=1.0 and max_samples=1.0 then all the base classifiers will probably be similar so there will be no point in combining them.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
