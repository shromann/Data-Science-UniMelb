{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 12 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are talking about Unsupervised Machine Learning Methods.\n",
    "\n",
    "We are going to implement and evaluate some clustring methods using k-Means, GMM and KDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "In this section, we'll write a function to generate a synthetic data set with Normal distributions.\n",
    "Later on, we'll try to fit k-Means and GMM to the synthetic data set and see how well we can recover our predifiend parameters.\n",
    "\n",
    "We'll store our predifined parameters (to develop our synthetic data) in NumPy arrays as follows. Note: the zero-th axis indexes the component $c$ for all arrays.\n",
    "* `weights`: a 1D array $[w_1, \\ldots, w_k]$\n",
    "* `means`: a 2D array $[\\mathbf{\\mu}_1, \\ldots, \\mathbf{\\mu}_k]$\n",
    "* `covariances`: a 3D array $[\\mathbf{\\sigma}_1, \\ldots, \\mathbf{\\sigma}_k]$\n",
    "\n",
    "Below are some example parameters for a 2D feature space ($m = 2$) with $k = 3$ components. Note that the covariance matrices must be symmetric positive semi-definite. Thus each covariance matrix only has 3 degrees of freedom (for $m = 2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "means = np.array([[0, 0],    # mean of 1st component\n",
    "                  [50, 60],  # mean of 2nd component\n",
    "                  [0, 100]]) # mean of 3rd component\n",
    "\n",
    "covariances = np.array([[[160, 20], [20, 180]],  # covariance matrix of 1st component\n",
    "                        [[170, 30], [30, 120]],  # covariance matrix of 2nd component \n",
    "                        [[130, 40], [40, 130]]]) # covariance matrix of 3rd component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (a)\n",
    "Complete the data generation function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_instances, weights, means, covariances):\n",
    "    \"\"\"\n",
    "    Generate data from a GMM\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    n_instances : int\n",
    "        number of instances in the generated data set\n",
    "    weights : numpy array, shape: (n_components,)\n",
    "        normalised component weights\n",
    "    means : numpy array, shape (n_components, n_features)\n",
    "        component means\n",
    "    covariances : numpy array, shape (n_components, n_features, n_features)\n",
    "        component covariance matrices\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    numpy array, shape (n_instances, n_features)\n",
    "        data matrix\n",
    "    \"\"\"\n",
    "    n_components, n_features = means.shape\n",
    "    data = np.empty((0, n_features), dtype=np.double)\n",
    "    \n",
    "    # Draw number of instances in each component\n",
    "    counts = ... \n",
    "    \n",
    "    for c in range(0, n_components):\n",
    "        # Draw x_i's for this component\n",
    "        cData = ...\n",
    "        \n",
    "        # Append to data\n",
    "        data = ... \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(200, weights, means, covariances)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (b)\n",
    "Use the method of k-means to cluster this data. Show the Centroids. Are they close to our predefined parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "km = ...\n",
    "...\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.scatter(centers[:,0],centers[:,1])\n",
    "plt.show()\n",
    "\n",
    "centers = ...\n",
    "print('cluster centers:\\n', centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (c)\n",
    "Now use the GMM method to cluster this data. Show the mean and Standard Deviations. Are they close to our predefined parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "gmm = ...\n",
    "...\n",
    "\n",
    "means = ...\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.scatter(means[:,0],means[:,1])\n",
    "plt.show()\n",
    "\n",
    "print('weights:\\n {}\\n'.format(...))\n",
    "print('means:\\n {}\\n'.format(...))\n",
    "print('covariances:\\n {}\\n'.format(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. \n",
    "For 2D data, we can also visualise the fitted model. \n",
    "The 2D Gaussians can be represented with isoline ellipsoids. \n",
    "For each Gaussian component, the ellipsoid is a location of points that have the same probability. \n",
    "\n",
    "Plotting an ellipsoid for a given 2D Gaussian, is somewhat non-trivial, and we are going to use a function developed for this purpose. \n",
    "Understanding the code and theory of function *plot_cov_ellipse* is not necessary for this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellipse-plotting-in-python/\n",
    "# and https://github.com/joferkington/oost_paper_code/blob/master/error_ellipse.py\n",
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, fc='none', ec=[0,0,0], a=1, lw=2):\n",
    "    \"\"\"\n",
    "    Plots an `nstd` sigma error ellipse based on the specified covariance\n",
    "    matrix (`cov`). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        nstd : The radius of the ellipse in numbers of standard deviations.\n",
    "            Defaults to 2 standard deviations.\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "        Additional keyword arguments are pass on to the ellipse patch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A matplotlib ellipse artist\n",
    "    \"\"\"\n",
    "    from scipy.stats import chi2\n",
    "    from matplotlib.patches import Ellipse\n",
    "    \n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    \n",
    "    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':a, 'linewidth':lw}\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(a)\n",
    "Using the above function, implement visualisation that plots data overlaid with fitted Gaussian ellipsoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm(data, gmm):\n",
    "    \"\"\"\n",
    "    data : numpy array, shape: (n_instances, n_features)\n",
    "        data matrix\n",
    "    \n",
    "    gmm : GaussianMixture\n",
    "        GaussianMixture instance to use for predictions/plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.scatter(...)\n",
    "    for c in range(gmm.n_components):\n",
    "        plot_cov_ellipse(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm(data, gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(b)\n",
    "Let's see what happens if we specify the \"wrong\" number of clusters. Use GMM to divide the data in 2, 5 and 9 clusters and illustrate the results. What is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_2 = ...\n",
    "plot_gmm(data, gmm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_5 = ...\n",
    "plot_gmm(data, gmm_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_9 = ...\n",
    "plot_gmm(data, gmm_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "In the previous section, we saw that it's important to select an appropriate value for $k$â€”i.e. GMM is not reslient to misspecified $k$.\n",
    "But how we can find the \"correct\" k in a realistic situation (when the data is not synthetic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 3.(a)\n",
    "Use **Log-likelihood** for selecting $k$. Log-likelihood of GMM can be computed for a data matrix `X` using `gmm.score(X)`.\n",
    "\n",
    "For this task we need to divide our data to train and evaluation datasets (why?).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(200, weights, means, covariances)\n",
    "\n",
    "train_data, validation_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "plt.scatter(..., marker='.', label='Train')\n",
    "plt.scatter(..., marker='*', label='Validation')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit a GMM for each value of $k \\in \\{1,\\ldots, 10\\}$ and compute:\n",
    "* `train_ll`: log-likelihood on the training set\n",
    "* `validation_ll`: log-likelihood on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_score(rng,data):\n",
    "    \n",
    "    range_k = np.arange(1, rng, dtype=int)\n",
    "    n_instances = data.shape[0]\n",
    "    \n",
    "    # Arrays to hold quantities for each k\n",
    "    train_ll = np.zeros(range_k.size)\n",
    "    validation_ll = np.zeros(range_k.size)\n",
    "  \n",
    "    for i,k in enumerate(range_k):\n",
    "        gmm_cv = ...\n",
    "        train_ll[i] = ...\n",
    "        validation_ll[i] = ...\n",
    "    \n",
    "    return range_k, train_ll, validation_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = 10\n",
    "range_k, train_ll, validation_ll = gmm_score(rng,data)\n",
    "\n",
    "plt.plot(range_k, train_ll, 'b.-', label = 'Train')\n",
    "plt.plot(range_k, validation_ll, 'k.-', label = 'Validation')\n",
    "plt.xlabel('number of components, $k$')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(b)\n",
    "What is the best K based on the above diagram? Is it compatible with our predifined parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(c)\n",
    "Let's try it again with a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.3, 0.2, 0.3, 0.1, 0.1])\n",
    "\n",
    "means = np.array([[0, 0], \n",
    "                  [50, 60], \n",
    "                  [0, 100], \n",
    "                  [100, -20], \n",
    "                  [-20, 40]])\n",
    "\n",
    "covariances = np.array([[[160, 20], [20, 180]], \n",
    "                        [[170, 30], [30, 120]], \n",
    "                        [[130, 40], [40, 130]], \n",
    "                        [[130, 40], [40, 130]], \n",
    "                        [[130, 40], [40, 130]]])\n",
    "\n",
    "data = generate_data(200, weights, means, covariances)\n",
    "\n",
    "train_data, validation_data = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(..., marker='.', label='Train')\n",
    "plt.scatter(..., marker='*', label='Validation')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = 10\n",
    "range_k, train_ll, validation_ll = gmm_score(rng,data)\n",
    "\n",
    "plt.plot(range_k, train_ll, 'b.-', label = 'Train')\n",
    "plt.plot(range_k, validation_ll, 'k.-', label = 'Validation')\n",
    "plt.xlabel('number of components, $k$')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(d)\n",
    "Analyse the resulting plots. What can you tell about the number of parameters? Can all of these quantities be used to estimate the number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.\n",
    "\n",
    "### Exercise 4.(b)\n",
    "\n",
    "Use kernel density estimation on the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian', bandwidth=10)\n",
    "kde.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw figure (heatmap or isocontours)\n",
    "def drawPDF(pdf, xlim, ylim):\n",
    "\n",
    "    plt.imshow(pdf,\n",
    "        origin='lower', aspect='auto',\n",
    "        extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
    "        cmap='Reds')\n",
    "    plt.scatter(train_data[:,0], train_data[:,1], marker='.')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.show()\n",
    "\n",
    "    levels = np.linspace(0, pdf.max(), 5)\n",
    "    plt.contour(Xgrid, Ygrid, pdf,\n",
    "        origin='lower',\n",
    "        colors='black')\n",
    "    plt.scatter(train_data[:,0], train_data[:,1], marker='.')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample points on a regular grid to show the distribution\n",
    "xlim = np.array([-50,125])\n",
    "ylim = np.array([-50,125])\n",
    "Xgrid, Ygrid = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
    "samples = ...\n",
    "samples = ...\n",
    "\n",
    "# probability estimate at the sample points\n",
    "pdf = ...\n",
    "pdf = ...\n",
    "drawPDF(pdf, xlim, ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.(b)\n",
    "\n",
    "The kernel bandwidth is a free parameter that affects the smoothness of the probability distribution. What bandwidth range will produce 4 separate peaks corresponding to the 4 clusters? What happens if you increase or decrease the bandwidth beyond this range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian', bandwidth=60)\n",
    "kde.fit(train_data)\n",
    "pdf = ...\n",
    "pdf = ...\n",
    "drawPDF(pdf, xlim, ylim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
