{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 7 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will be examining the behaviour of **Linear Model.**\n",
    "\n",
    "well-known _Boston Housing Dataset_ containing data about housing suburbs in Boston, Massachuetts. There are **13 features** in this dataset which are intended to be used to predict the target, the median house value in the given suburb, `MEDV`. \n",
    "\n",
    "For simplicity we'll only work with a single variable, the *\"percentage of the population in the suburb classified as 'lower status' by the US Census service in 1978\"*, `LSTAT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "Lets first plot `LSTAT` vs. `MEDV` in the *Boston Dataset* to see if a linear model can be plausible model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIGURE_RESOLUTION = 128\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "ds = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "features = ['LSTAT'] #  ds.columns  # ['RM', 'LSTAT']\n",
    "target = 'MEDV'\n",
    "\n",
    "ds['LSTAT'] = ds['LSTAT'].apply(lambda x: x/100.)\n",
    "\n",
    "for f in features:\n",
    "    plt.figure()\n",
    "    plt.scatter(ds[f], boston.target, marker='.')\n",
    "    plt.xlabel(...)\n",
    "    plt.ylabel(...)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (a)\n",
    "It seems like there is a (roughly) linear relation between these two features is visible. Let's train a Linear Regression to model the relation between the `LSTAT` and `MEDV`.\n",
    "\n",
    "To do so we first split our dataset to two groups of `TRAIN` and `TEST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = boston.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_full_train, x_full_test, y_train, y_test = train_test_split(ds, Y, test_size=0.2, random_state=30027)\n",
    "\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(..., ...))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the functionality of `LinearRegression` in `sklearn` to train our model, but first let's talk about the theoritical concepts of the Linear model.\n",
    "\n",
    "In lectures, we saw that a linear model can be expressed as:\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j = 1}^{m} w_j x_j = \\mathbf{w} \\cdot \\mathbf{x} $$\n",
    "where \n",
    "\n",
    "* $y$ is the *target variable*;\n",
    "* $\\mathbf{x} = [x_0, x_1, \\ldots, x_m]$ is a vector of *features* (we define $x_0 = 1$ which is our bias); and\n",
    "* $\\mathbf{w} = [w_0, \\ldots, w_m]$ are the *weights*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in lectures that for finding the optimum weights we can *minimize*  the following mean squared errors:  \n",
    "$$E(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to solve for the optimal weights $\\mathbf{w}^\\star$ analytically by solving for $\\nabla_{\\mathbf{w}} E(\\mathbf{w}) = 0$. This yields the _normal equations_ for the least squares problem:\n",
    "$$\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "where $X$ is the matrix of attributes, which is also called _design matrix_. In our simple 1-feature case this is:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{pmatrix} \n",
    "        1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \n",
    "    \\end{pmatrix} \n",
    "  \\quad \\text{and} \\quad \n",
    "  \\mathbf{y} = \\begin{pmatrix} \n",
    "          y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Datastructure for the X_train, X_test and Y _Design Matrices_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones_like(x_full_train[features]), x_full_train[features].values.ravel()))\n",
    "X_test = np.column_stack((np.ones_like(x_full_test[features]), x_full_test[features].values.ravel()))\n",
    "\n",
    "Y_train = ...\n",
    "Y_test = ...\n",
    "\n",
    "print('X_train Design matrix shape:', X_train.shape)\n",
    "print('Y_train Design matrix shape:', Y_train.shape)\n",
    "\n",
    "print('X_test Design matrix shape:', X_test.shape)\n",
    "print('Y_test Design matrix shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LinearRegression module in _sklearn_, fitting a linear regression is just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(...,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` module provides access to the bias weight $w_0$ under the `intercept_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the non-bias weights under the `coef_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 1. (b)\n",
    "Show your trained model in a plot. Do you think it is a good fit? How can it be better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(...)\n",
    "\n",
    "plt.scatter(X_test[:,1], Y_test, marker='.')\n",
    "\n",
    "plt.plot(...,...)\n",
    "\n",
    "plt.xlabel(f)\n",
    "plt.ylabel('Median House Value (thousands)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if it is a good model or not, we can use our error function. In the lectures the introduced Error(Loss) function for Linear Regression is Mean Squared Error (MSE).\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(Y_test,...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (c)\n",
    "What happens to our model if we use the other 12 variables available in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "lr_full = LinearRegression().fit(x_full_train, y_train)\n",
    "\n",
    "y_train_pred = lr_full.predict(...)\n",
    "y_test_pred = lr_full.predict(...)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train,...))\n",
    "print('Test MSE:', mean_squared_error(y_test,...))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "While linear regression is extremely effective in the right context, the formulation above, where we model the target $y = \\mathbf{w} \\cdot \\mathbf{x}$ is a straight line (in a 2D dimention) which may be limit in expressing our data. \n",
    "\n",
    "We can increase the flexibility of our model by introducing nonlinear basis functions of the input variables, $\\vec{\\phi}(\\mathbf{x})$ (Phi of X). \n",
    "\n",
    "Now our predictor $y = \\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x})$ is a nonlinear function of the input $\\mathbf{x}$ (but still linear in the model parameters $\\mathbf{w}$). There are many possible choices for the basis $\\vec{\\phi}(\\mathbf{x})$, but we will focus on using polynomial basis functions with the form $\\phi(x) = x^j$ in the single-variable case. \n",
    "\n",
    "All our previous results for linear regression carry over as you would expect, we replace $\\mathbf{w} \\cdot \\mathbf{x}$ with $\\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x})$ - the error function and normal equations become:\n",
    "\n",
    "\\begin{equation}\n",
    "    E(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x}_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}^* = \\left[\\mathbf{\\Phi}^\\top \\mathbf{\\Phi}\\right]^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Here the design matrix $\\Phi \\in \\mathbb{R}^{n \\times (m+1)}$ for $n$ datapoints and $m$ basis functions, $\\vec{\\phi}(\\mathbf{x}) = (\\phi_0(\\mathbf{x}), \\ldots \\phi_m(\\mathbf{x}))$. In the case of polynomial regression $\\vec{\\phi}(x) = \\left(1,x,x^2,\\ldots, x^m\\right)$ (note the one additional bias parameter). For example, for a maximum polynomial order of 3, the design matrix is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi} = \\begin{pmatrix} \n",
    "        1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 & x_n^3\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. (a)\n",
    "Lets start by building the design matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def design_matrix_poly(x, x_test, order):\n",
    "    \"\"\"\n",
    "    Returns design matrix on training and testing data\n",
    "    for polynomial basis of specified order\n",
    "    \"\"\"\n",
    "    n, n_test, m = x.shape[0], x_test.shape[0], order + 1\n",
    "    Phi = np.zeros([n,m])\n",
    "    Phi_test = np.zeros([n_test,m])\n",
    "\n",
    "    for k in range(order+1):\n",
    "        Phi[:,k] = np.power(..., k)\n",
    "        Phi_test[:,k] = np.power(...,k)\n",
    "    \n",
    "    return Phi, Phi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_order = 3\n",
    "Phi, Phi_test = design_matrix_poly(x, x_test, order=start_order)\n",
    "\n",
    "print('Phi_train Design matrix shape:', Phi.shape)\n",
    "\n",
    "print('Phi_test Design matrix shape:', Phi_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. (b)\n",
    "Train the model using the Phi and analyse the weights ($\\mathbf{w}$) of the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lr_poly = LinearRegression().fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. (c)\n",
    "Now let's solve the linear system using the inbuilt `numpy` routine and plot the result of the predicted values on the training set. \n",
    "\n",
    "**NOTE:** For teaching purpose, in this exercise we are not using the `PolynomialFeatures` module in `sklearn`. You can use the functions of `PolynomialFeatures` module if you want to use this model in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(X_test, w): \n",
    "    return np.dot(X_test, w)  # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(x_test, y_test, x, y):\n",
    "    # To get connecting lines in matplotlib correct\n",
    "    sort_idx = np.argsort(x_test)\n",
    "    y_test = y_test[sort_idx]\n",
    "    x_test = np.sort(x_test)\n",
    "    plt.plot(x_test, y_test, 'b-')\n",
    "    plt.scatter(x, y, marker='.')\n",
    "    plt.ylabel(\"$y$ (Median House Price)\")\n",
    "    plt.xlabel(\"$x$ (LSTAT)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(X, w, y):\n",
    "    return np.mean((np.dot(X, w) - y)**2)  # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_poly = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
    "plot_fit(x, predict(Phi, w_poly), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets scan across a range of powers. What do you expect to happen as we increase the maximum polynomial order on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = list(range(12))\n",
    "Phi_test_orders = list()\n",
    "w_orders = list()\n",
    "mse_orders = list()\n",
    "\n",
    "sort_idx = np.argsort(x)\n",
    "y_plot = y[sort_idx]\n",
    "x_plot = np.sort(x)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi, Phi_test = design_matrix_poly(x, x_test, order=order)\n",
    "    w_poly = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
    "    mse = mean_squared_error(Phi, w_poly, y)\n",
    "\n",
    "    Phi_test_orders.append(Phi_test)\n",
    "    w_orders.append(w_poly)\n",
    "    mse_orders.append(mse)\n",
    "    \n",
    "    plt.plot(x_plot, predict(Phi, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(..., ...))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (train) with different maximum polynomial orders.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll repeat on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test_orders = list()\n",
    "sort_idx = np.argsort(x_test)\n",
    "y_plot = y_test[sort_idx]\n",
    "x_plot = np.sort(x_test)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi_test = Phi_test_orders[i]\n",
    "    w_poly = w_orders[i]\n",
    "    \n",
    "    mse_test = mean_squared_error(Phi_test, w_poly, y_test)\n",
    "    mse_test_orders.append(mse_test)\n",
    "    \n",
    "\n",
    "    plt.plot(x_plot, predict(Phi_test, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(..., ...))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (Test) with different maximum polynomial orders.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot MSE vs. polynomial order for the training data and held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(mse_orders)), mse_orders, label='Train')\n",
    "plt.plot(range(len(mse_test_orders)), mse_test_orders, label='Test')\n",
    "plt.title('Mean squared error vs. Maximum polynomial order')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Maximum polynomial order')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
