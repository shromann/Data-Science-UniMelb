{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 \n",
    "\n",
    "## Week 4 - Sample Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be using scikit-learn to classify some data, and to evaluate some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Please load Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the gold–standard (“ground truth”) labels is called y ; create these variables for the car data.\n",
    "\n",
    "- **(a)** Load the data into a suitable format for scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('car.data', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        X.append(atts[:-1]) #all atts, excluding the class\n",
    "        y.append(atts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How many instances are there in this collection? How many attributes, and of what type(s)? What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1728 instances\n",
      "There are 6 attributes, for example: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "There are 4 class labels: {'vgood', 'good', 'acc', 'unacc'}\n",
      "Label frequencies: [('unacc', 1210), ('acc', 384), ('good', 69), ('vgood', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('There are', len(X), 'instances')\n",
    "print('There are', len(X[0]), \"attributes, for example:\", X[0])\n",
    "print('There are', len(set(y)), \"class labels:\", set(y))   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies: %s\" %str(label_counter.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "- **(a)** Write some functions that transform our **categorical** attributes into **numerical** attributes, by (perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "```python\n",
    "def convert_class(raw):\n",
    "    if raw==\"unacc\": return 0\n",
    "    elif raw==\"acc\": return 1\n",
    "    elif raw==\"good\": return 2\n",
    "    elif raw==\"vgood\": return 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1: {'high', 'vhigh', 'med', 'low'}\n",
      "feature 2: {'high', 'vhigh', 'med', 'low'}\n",
      "feature 3: {'4', '2', '5more', '3'}\n",
      "feature 4: {'4', '2', 'more'}\n",
      "feature 5: {'small', 'big', 'med'}\n",
      "feature 6: {'high', 'med', 'low'}\n"
     ]
    }
   ],
   "source": [
    "# We could check this from the \"car.names\" file linked above\n",
    "# Here's one (somewhat inefficient) way of reading this from the data itself\n",
    "feature_1_values = set([X[i][0] for i in range(len(X))])\n",
    "feature_2_values = set([X[i][1] for i in range(len(X))])\n",
    "feature_3_values = set([X[i][2] for i in range(len(X))])\n",
    "feature_4_values = set([X[i][3] for i in range(len(X))])\n",
    "feature_5_values = set([X[i][4] for i in range(len(X))])\n",
    "feature_6_values = set([X[i][5] for i in range(len(X))])\n",
    "print(\"feature 1: %s\" %str(feature_1_values))\n",
    "print(\"feature 2: %s\" %str(feature_2_values))\n",
    "print(\"feature 3: %s\" %str(feature_3_values))\n",
    "print(\"feature 4: %s\" %str(feature_4_values))\n",
    "print(\"feature 5: %s\" %str(feature_5_values))\n",
    "print(\"feature 6: %s\" %str(feature_6_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1728, 6), y shape: (1728,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_feature_1and2and6(raw):\n",
    "    if raw == \"low\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"high\": return 2\n",
    "    elif raw == \"vhigh\": return 3\n",
    "    # In general, we might want to catch unexpected values, too\n",
    "def convert_feature_3(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"3\": return 1\n",
    "    elif raw == \"4\": return 2\n",
    "    elif raw == \"5more\": return 3\n",
    "def convert_feature_4(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"4\": return 1\n",
    "    elif raw == \"more\": return 2\n",
    "def convert_feature_5(raw):\n",
    "    if raw == \"small\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"big\": return 2\n",
    "def convert_class(raw):\n",
    "    if raw == \"unacc\": return 0\n",
    "    elif raw == \"acc\": return 1\n",
    "    elif raw == \"good\": return 2\n",
    "    elif raw == \"vgood\": return 3\n",
    "\n",
    "X_ordinal = []\n",
    "for x in X:\n",
    "    f1, f2, f3, f4, f5, f6 = x\n",
    "    f1 = convert_feature_1and2and6(f1)\n",
    "    f2 = convert_feature_1and2and6(f2)\n",
    "    f3 = convert_feature_3(f3)\n",
    "    f4 = convert_feature_4(f4)\n",
    "    f5 = convert_feature_5(f5)\n",
    "    f6 = convert_feature_1and2and6(f6)\n",
    "    x = [f1, f2, f3, f4, f5, f6]\n",
    "    X_ordinal.append(x)\n",
    "    \n",
    "#convert to int array to make sure everything is converted.\n",
    "X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "\n",
    "#convert ys\n",
    "y_numeric = []\n",
    "for this_y in y:\n",
    "    this_y = convert_class(this_y)\n",
    "    y_numeric.append(this_y)\n",
    "\n",
    "y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "\n",
    "print('X shape: {}, y shape: {}'.format(X_ordinal.shape, y_num.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Load the dataset again, this time as integers. Observe that we can actually build a model using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_ordinal, y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Split the data into training (80%) and test sets (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1157, 6) X_test: (571, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Read up on different implementations of the Naive Bayes classifier in `sklearn.naive_bayes`. Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Compare the accuracies of all three different kinds of Naive Bayes classifier. Does this accord with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB score 0.686515 \n",
      "MNB score 0.711033 \n",
      "BNB score 0.763573 \n",
      "GNB score 0.709282 \n",
      "MNB score 0.705779 \n",
      "BNB score 0.814361 \n",
      "GNB score 0.719790 \n",
      "MNB score 0.718039 \n",
      "BNB score 0.779335 \n",
      "Avg GNB score: 0.705195563339171\n",
      "Avg MNB score: 0.7116170461179219\n",
      "Avg BNB score: 0.7857559836544074\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "##print(dir(nb))\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It's no real surprise that Multinomial NB doesn't work here; for example \"high\" (2), is not really \"medium\" (1) repeated twice.*\n",
    "\n",
    "*We might have expected that Gaussian NB would work a little bit here, but the ordering appears to be less significant than the feature values themselves. A secondary concern might the uniform distribution of attribute values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** By default, this implementation of Naive Bayes uses Laplace smoothing. Turn this off, and see what happens — what is the significance of the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score 0.711033 \n",
      "BNB score 0.763573 \n",
      "MNB score 0.705779 \n",
      "BNB score 0.814361 \n",
      "MNB score 0.718039 \n",
      "BNB score 0.779335 \n",
      "Avg MNB score: 0.7116170461179219\n",
      "Avg BNB score: 0.7857559836544074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Gaussian NB doesn't use smoothing; all of the probabilities for the Gaussian are already non-zero\n",
    "# You can try this for yourself, but scikit-learn will flatly refuse to do it\n",
    "#mnb = MultinomialNB(alpha=0)\n",
    "#bnb = BernoulliNB(alpha=0)\n",
    "mnb = MultinomialNB(alpha=1.0e-10)\n",
    "bnb = BernoulliNB(alpha=1.0e-10)\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to the implementation (as log-probabilities), numerical errors would result from unseen events.*\n",
    "\n",
    "*This is now add-k smoothing, for a very small k. You can see that the predictions are largely the same for this particular dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** What happens if you increase the smoothing parameter instead? Calculate the accuracy for a range of values from 5 to 500. For the very large values, examine the predicted classes for the test instances — what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score 0.698774 \n",
      "BNB score 0.698774 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Let's not mess around, and go straight to a large value:\n",
    "mnb = MultinomialNB(alpha=500)\n",
    "bnb = BernoulliNB(alpha=500)\n",
    "\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For large values of the smoothing parameter, every instance is predicted to be the majority-class - effectively, this is the same behaviour as 0-R!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a strategy does seem reasonable in light of the given values (such as *small, med, big*).\n",
    "A different strategy would be to `binarise` the attributes: to replace a categorical attribute having `m` values with `m binary attributes`. One way of doing this in scikit-learn is using the **OneHotEncoder** :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Check the shape of `X_trans` — how many attributes do we have now? Does this correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 21)\n",
      "X[0]: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "X_trans[0]: [ 0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_ordinal)\n",
    "X_trans = ohe.transform(X_ordinal).toarray()\n",
    "\n",
    "print(X_trans.shape)\n",
    "print('X[0]:', X[0])\n",
    "print('X_trans[0]:', X_trans[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the dataset comprised of `one–hot attributes` into **train** and **test** sets. Compare the accuracies of the three Naive Bayes models using ordinal attributes with the three models using `one–hot attributes`: are you surprised? What can we infer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB score 0.793345 \n",
      "MNB score 0.816112 \n",
      "BNB score 0.837128 \n",
      "GNB score 0.824869 \n",
      "MNB score 0.865149 \n",
      "BNB score 0.891419 \n",
      "GNB score 0.789842 \n",
      "MNB score 0.814361 \n",
      "BNB score 0.858144 \n",
      "Avg GNB score: 0.8026853473438411\n",
      "Avg MNB score: 0.8318739054290717\n",
      "Avg BNB score: 0.8622300058377116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a fairly drastic difference: Bernoulli NB is still the best option, but both Gaussian and Multinomial NB are no longer useless. It appears that all of these learners can identify meaningful patterns, just by taking the attribute value in isolation (and not in relation to the presumed ordering) - and so, perhaps our original assignment of 0,1,2,3 was too simple to discover patterns.*\n",
    "\n",
    "*At this point, we can also observe that the default behaviour of scikit-learn's Bernoulli NB is to do ... something ... with non-binary attributes, but it is usually better to make them explicitly binary using the one-hot transformer. (If you're curious, in this case, it's treating whichever value is 0 as \"N\", and the other values as \"Y\".)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
