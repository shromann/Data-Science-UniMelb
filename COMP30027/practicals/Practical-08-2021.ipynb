{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Week 8 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we first examine **Logistic Regression** classifier. Then we will use many of the classifier models that we covered so far to build different ensembles and analyse the outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "Let's start with *Logistic Regression*. Use the IRIS dataset (again) and train a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=88)\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(..,..)\n",
    "print(\"Accuracy:\",lgr.score(..,..))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (a)\n",
    "Now using the same split, compare the results form Logistic Regression with other classifiers we covered so far. You may use: Zero-R, Gaussian Naive Bayes, Multinomial Naive Bayes,linear SVM, kNN and Decision Tree.\n",
    "\n",
    "Compare their accuracy and the time required for prediction. Analyse the results.\n",
    "\n",
    "Note: Please use the classifiers default hyper parameters (No tunning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          ...\n",
    "          DecisionTreeClassifier(),\n",
    "          KNeighborsClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'KNN',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(X_train,y_train)\n",
    "    start = time.time()\n",
    "    acc = ...\n",
    "    end = time.time()\n",
    "    t = ...\n",
    "    print(title, \"Accuracy:\",acc, 'Time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `sklearn` is not happy with us when using Linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.(b)\n",
    "Do the same comparision using the 10-fold Cross-Validation evaluation strategy. Analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = ...\n",
    "    end = time.time()\n",
    "    t = ...\n",
    "    print(title, \"Accuracy:\",acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Getting to the concept of *stacking*. We want train a meta-classifier (level-1 model) over the outputs of the base classifiers (level-0 model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(a)\n",
    "Using the IRIS dataset, build a stacking of the classifiers:\n",
    "- Zero_R\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Gaussian NB\n",
    "- Multinomial NB\n",
    "- Decsion Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn does support stacking. Check the followig: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "\n",
    "However, to have a better understanding of Stacking, we can implement it by ourselves based on the following steps:\n",
    "- We need to train each of our models (using fit()),\n",
    "- And then classify each training instance (using predict()),\n",
    "- We build up a matrix where the instances are composed of attributes, which correspond to the predictions of each model on this training instance1.\n",
    "- We then train our final learner on this matrix of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You should think about which classifier is most suited to being the final metaâ€“classifier in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [...]\n",
    "titles = [...]\n",
    "\n",
    "meta_classifier = ...\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IRIS dataset\\n\")\n",
    "\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(title, \"Accuracy:\",...)\n",
    "    \n",
    "stacker.fit(X_train, y_train)\n",
    "print('\\nStacker Accuracy:', ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Exercise 2.(b)\n",
    "Use the same stack to process the `car` dataset use holdout strategy with 30% split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_data(i_file):\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data('car.data')\n",
    "\n",
    "#print('labels:', set(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n",
    "\n",
    "print(\"Car dataset\\n\")\n",
    "\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(title, \"Accuracy:\",...)\n",
    "    \n",
    "stacker.fit(X_train, y_train)\n",
    "print('\\nStacker Accuracy:', ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Bagging is often associated with Decision Trees, but in scikit-learn , it can be applied to any learner. \n",
    "\n",
    "If we use bagging with Decision Tree, we will build a number of Decision Trees by re-sampling the data:\n",
    "- For each tree, we randomly select (with repetition) N instances out of the possible N instances, so that we have the same sized data as the deterministic decision tree, but each one is based around a different data set\n",
    "- We then build the tree as usual.\n",
    "- We classify the test instance by **voting** - each tree gets a vote (the class it would predict for the test instance), and the class with the plurality wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(a)\n",
    "Load the `lymphography` dataset and implement bagging of 10 estimator for kNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN bagging Accuracy: 0.7618213660245184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "X, y = load_data('lymphography.data')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "bagging = BaggingClassifier(base_estimator=...,n_estimators=..., max_samples=0.5, max_features=0.5)\n",
    "\n",
    "KNN.fit(X_train,y_train)\n",
    "bagging.fit(X_train,y_train)\n",
    "\n",
    "print(\"KNN:\",...)\n",
    "print(\"KNN Bagging Accuracy:\",...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the documentation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n",
    "\n",
    "Bagging classifier builds N base classifier, each base classifier is trained/fitted on a subset of features/samples. For each base classifier:\n",
    "\n",
    "- Randomly select max_features * X.shape[1] subset of features.\n",
    "- Randomly select max_samples * X.shape[0] subset of samples.\n",
    "- Create a new X_base from the selected features and samples.\n",
    "- Fit the base classifier on X_base and y_base.\n",
    "\n",
    "Then use Voting or averaging to combine the prediction of the base classifier for X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(b)\n",
    "What are the significance of max_samples and max_features , and why might we wish to use values less than 1.0?\n",
    "\n",
    "Build 3 differnt Decision Tree bagging classifiers using differnt combinations of max_samples and max_features. Can you analyse the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "bagging_one = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=..., max_features=...)\n",
    "bagging_two = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=..., max_features=...)\n",
    "bagging_three = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n",
    "                              max_samples=..., max_features=...)\n",
    "\n",
    "DT.fit(X_train,y_train)\n",
    "bagging_one.fit(X_train,y_train)\n",
    "bagging_two.fit(X_train,y_train)\n",
    "bagging_three.fit(X_train,y_train)\n",
    "\n",
    "print(\"DT:\",DT.score(X_test,y_test))\n",
    "print(\"Option 1: bagging Accuracy:\",bagging_one.score(X_test,y_test))\n",
    "print(\"Option 2: bagging Accuracy:\",bagging_two.score(X_test,y_test))\n",
    "print(\"Option 3: bagging Accuracy:\",bagging_three.score(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
