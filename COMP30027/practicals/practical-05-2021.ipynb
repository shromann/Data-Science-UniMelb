{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 5 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will be using `scikit-learn` again to learn more about Baselines and Decision Trees.\n",
    "### Exercise 1. \n",
    "Load the `Iris` dataset as follows (Note that there are some small differences between this dataset and the one we were looking at last week, most notably, the errors that we needed to fix are not present):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Identify the contents of the complex data type iris , for example iris.DESCR contains a long description of the dataset, which you can `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the “ground truth” labels is called y ; create these variables for the Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Confirm that X is a 2-dimensional array, with a row for each instance and a column for each attribute. (Hint: read about the `shape` property in numpy .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "Let’s build a 0-R classifier (“majority class classifier”). In scikit-learn , this is a `DummyClassifier`. \n",
    "\n",
    "**Note** `scikit-learn` uses this terminology to help remind you not to use these sorts of classifiers when trying to solve real problems; However they are easy **baseline classifiers** and are quite useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "zero_r = DummyClassifier(strategy='most_frequent')\n",
    "zero_r.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Confirm that this is a typical 0-R classifier by checking its predictions on the training data: `zero_r.predict(X)` — which class has it chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = ...\n",
    "print(ybar)\n",
    "label_counter = Counter(y)\n",
    "label_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is a number, as far as scikit-learn is concerned - so it's a little difficult to know which class label this is. On the other hand, each of the classes is equally likely, so the method appears to have chosen the \"0th\" class arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** The default evaluation metric associated with a `DummyClassifier` is `accuracy`, which you can observe using `score()` , for example: `zero_r.score(X, y)`. This strategy — building a model, and then evaluating on the data that we used to build the model — gives us something called “training accuracy”, and is generally frowned upon in the *Machine Learning* community. Why do you suppose this is? (We’ll examine some better techniques later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_r.score(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Contrast the `0-R classifier` with the “weighted random classifier”, which makes random predictions according to the distribution of classes in the training data; (`strategy='stratified'`) — check its predictions, and evaluate its training accuracy. Does it have a higher accuracy, on average, than `0-R`, or a `lower accuracy`? (You should run `score()` at least 10 times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_clf = DummyClassifier(strategy='stratified')\n",
    "stratified_clf.fit(...)\n",
    "accuracies = []\n",
    "num_runs = 10\n",
    "for i in range(num_runs):\n",
    "    acc = stratified_clf.score(...)\n",
    "    accuracies.append(acc)\n",
    "print(accuracies)\n",
    "print('Average accuracy over {} runs is: {}.'.format(num_runs, np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Let’s consider a couple of other classifiers: a `Decision Tree`, and `1-R` (which is really just a limited\n",
    "`DecisionTreeClassifier` in `scikit-learn` ).\n",
    "\n",
    "**NOTE:** `scikit-learn` implementation of `1-R` is slightly different to the lecture version, because it doesn’t count errors — rather it uses the Gini coefficient or the Information Gain to determine the best attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "one_r = DecisionTreeClassifier(max_depth=1)\n",
    "one_r.fit(X, y)\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Find the training accuracy of the two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_r_acc = ...\n",
    "dt_acc = ...\n",
    "print(\"1-R accuracy: {}; DT accuracy: {}\".format(one_r_acc, dt_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** The \"`feature_importances_`\" attribute is adequate for completely describing the 1-R classifier. Which attribute is being used to classify the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = one_r.feature_importances_\n",
    "max_index = ...\n",
    "best_feature_name = ...\n",
    "print(best_feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** *(Harder)* Check the predicted labels for each instance to discern the values for this attribute that each class maps to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = one_r.predict(X)\n",
    "best_feature = X[:, max_index]\n",
    "plt.scatter(...)\n",
    "plt.xlabel(best_feature_name)\n",
    "plt.ylabel('predicted class')\n",
    "plt.show()\n",
    "#print(ybar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(d)** The default splitting criterion for these Decision Trees is the **Gini coefficient**. Read up on the difference between this and the **Information Gain** — do you expect the behaviour of this model to change by using the alternative splitting criterion? Try it, and confirm your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_r = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "one_r.fit(X, y)\n",
    "dt = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\")\n",
    "dt.fit(X, y)\n",
    "\n",
    "one_r_acc = ...\n",
    "dt_acc = ...\n",
    "print(\"Information Gain/entropy: 1-R accuracy: {} DT accuracy: {}\".format(one_r_acc, dt_acc))\n",
    "\n",
    "importances = one_r.feature_importances_\n",
    "max_index = np.argmax(importances)\n",
    "best_feature_name = iris.feature_names[max_index]\n",
    "print(\"1-R attribute: \",best_feature_name)\n",
    "\n",
    "ybar = one_r.predict(X)\n",
    "best_feature = X[:, max_index]\n",
    "plt.scatter(best_feature, ybar, c=ybar)\n",
    "plt.xlabel(best_feature_name)\n",
    "plt.ylabel('predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "A better mechanism for evaluating a classifier is based on randomly partitioning the data into a\n",
    "training set and test set (the “holdout” method). There is an in-built utility for this in scikit-learn ,\n",
    "but it can be in one of two places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print('X_train: {} X_test: {} y_train: {} y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Train the three classifiers (`0-R, 1-R, Decision Tree`) on the training data, rather than the full data set. score() is too specific to be used in most situations; another way to find the training accuracy is by comparing the predictions to the ground truth labels as follows:\n",
    "\n",
    "```python\n",
    ">>> from scikit-learn.metrics import accuracy_score\n",
    ">>> accuracy_score(zero_R.predict(X_train),y_train))\n",
    "```\n",
    "\n",
    "- Calculate the accuracy of the classifiers on the held-out training data. How does it compare to the training accuracies you calculated before? Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "zero_r.fit(X_train, y_train)\n",
    "one_r.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "zr_acc = ...\n",
    "or_acc = ...\n",
    "dt_acc = ...\n",
    "print('Train accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Instead of calculating the accuracy with respect to the training set, train your classifiers on the training data (using `fit()`) and then evaluate them (by calculating accuracy) according to their predictions on the test data. How different are the training accuracies and test accuracies? Hypothesise what could be causing these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zr_acc = ...\n",
    "or_acc = ...\n",
    "dt_acc = ...\n",
    "print('Test accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** By default, `train_test_split` uses 75% of the data as training, and 25% as test. This can be changed by passing an argument, for example, `test_size=0.5` means that we use 50% as training and 50% as test. Try some different values (perhaps multiple times) to see if you can observe the trade-off inherent in the model using this evaluation strategy.\n",
    "\n",
    "- **Note** The default behaviour of `train_test_split` is that the remainder of the data is used as training; this too can be altered, if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_size in [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]:\n",
    "    print('Running experiments with test set size: {}'.format(test_size))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    print('X_train: {} X_test: {} y_train: {} y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "    \n",
    "    zero_r.fit(X_train, y_train)\n",
    "    one_r.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    zr_acc = ...\n",
    "    or_acc = ...\n",
    "    dt_acc = ...\n",
    "    print('Train accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n",
    "\n",
    "    zr_acc = ...\n",
    "    or_acc = ...\n",
    "    dt_acc = ...\n",
    "    print('Test accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.\n",
    "*(Stratified)* `M–fold` cross-validation is so popular, `scikit-learn` has a utility that applies it directly.\n",
    "For example, 10–fold cross-validation of the `0-R` classifier proceeds as follows:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.model_validation import cross_val_score # Newer versions\n",
    ">>> from sklearn.cross_validation import cross_val_score # Older versions\n",
    ">>> cross_val_score(zero_R, X, y, cv=10)\n",
    "```\n",
    "**Note:** There are also simpler methods like `StratifiedKFold()` to generate the partitions, which you can then use to train and test the model yourself, if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** This method returns an array of the calculated evaluation metric (by default, accuracy) across the folds. Write a wrapper function which averages these values, so as to come up with a single score for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(zero_r, X, y, cv=10))\n",
    "\n",
    "def avg_score(clf, X, y, cv=10):\n",
    "    scores = ...\n",
    "    return np.mean(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How does the estimate of the accuracy of the various classifiers using cross-validation compare to the training accuracies and holdout accuracies you calculated above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [zero_r, one_r, dt]:\n",
    "    avg = avg_score(...)\n",
    "    print(clf)\n",
    "    print('Average CV accuracy', avg)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
