{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Week 9 - Practical Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are talking about feature selection and text processing using in `scikit learn`. \n",
    "\n",
    "Remeber you can always use the Scikit Learn API (https://scikit-learn.org/stable/modules/classes.html#) for further exampes and explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. \n",
    "scikit-learn has an in-built text dataset, the “20 newsgroups corpus” https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html), which contains a number of documents classified with a topic, based on the newsgroup in which it was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. (a)\n",
    "Choose a couple of newsgroups that you think it would be interesting to discriminate between, like `rec.autos` and `rec.motorcycles`.\n",
    "\n",
    "We will use `alt.atheism` and `talk.religion.misc` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism','talk.religion.misc']\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "X_train_txt = data_train.data\n",
    "y_train = data_train.target\n",
    "X_test_txt = data_test.data\n",
    "y_test = data_test.target\n",
    "print(dir(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. (b)\n",
    "Examine a couple of documents, by referencing the list ( X_train[0] ). Can you accurately predict the class ( y_train[0] ) based on the text alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: acooper@mac.cc.macalstr.edu\n",
      "Subject: Re: Where are they now?\n",
      "Organization: Macalester College\n",
      "Lines: 38\n",
      "\n",
      "In article <1qi156INNf9n@senator-bedfellow.MIT.EDU>, tcbruno@athena.mit.edu (Tom Bruno) writes:\n",
      "> \n",
      "> Wow.  Leave your terminal for a few months and everyone you remember goes\n",
      "> away-- how depressing.  Actually, there are a few familiar faces out there,\n",
      "> counting Bob and Kent, but I don't seem to recognize anyone else.  Has anyone\n",
      "> heard from Graham Matthews recently, or has he gotten his degree and sailed\n",
      "> for Greener Pastures (tm)?  \n",
      "> \n",
      "> Which brings me to the point of my posting.  How many people out there have \n",
      "> been around alt.atheism since 1990?  I've done my damnedest to stay on top of\n",
      "> the newsgroup, but when you fall behind, you REALLY fall behind (it's still not\n",
      "> as bad as rec.arts.startrek used to be, but I digress).  Has anyone tried to\n",
      "> keep up with the deluge?  Inquiring minds want to know!  Also-- does anyone\n",
      "> keep track of where the more infamous posters to alt.atheism end up, once they\n",
      "> leave the newsgroup?  Just curious, I guess.\n",
      "> \n",
      "> cheers,\n",
      "> tom bruno\n",
      "\n",
      "\n",
      "I am one of those people who always willl have unlimited stores of unfounded\n",
      "respect for people who have been on newsgroups/mailing lists longer than I\n",
      "have, so you certainly have my sympathy Tom.  I have only been semi-regularly\n",
      "posting (it is TOUGHto keep up) since this February, but I have been reading\n",
      "and following the threads since last August: my school's newsreader was down\n",
      "for months and our incompetent computing services never bothered to find a new\n",
      "feed site, so it wasn't accepting outgoing postings.  I don't think anyone\n",
      "keeps track of where other posters go: it's that old love 'em and leave 'em\n",
      "Internet for you again...\n",
      "\n",
      "\n",
      "best regards,\n",
      "\n",
      "********************************************************************************\n",
      "* Adam John Cooper\t\t\"Verily, often have I laughed at the weaklings *\n",
      "*\t\t\t\t   who thought themselves good simply because  *\n",
      "* acooper@macalstr.edu\t\t\t\tthey had no claws.\"\t       *\n",
      "********************************************************************************\n",
      "\n",
      "label is 0 alt.atheism\n",
      "-----------------------------------------\n",
      "From: hedrick@athos.rutgers.edu (Charles Hedrick)\n",
      "Subject: Re: Clarification of personal position (Jesus and the Law)\n",
      "Organization: Rutgers Univ., New Brunswick, N.J.\n",
      "Lines: 98\n",
      "\n",
      "sandvik@newton.apple.com (Kent Sandvik) writes:\n",
      "\n",
      ">My online Bible is on a CD, but I don't own a CD-ROM system for the\n",
      ">time being, so I can't search for the famous cite where Jesus explicitly\n",
      ">states that he didn't want to break existing (Jewish) laws. In other\n",
      ">words technically speaking Christians should use Saturday and not Sunday\n",
      ">as their holy day, if they want to conform to the teachings of Jesus.\n",
      "\n",
      "I think the passage you're looking for is the following.\n",
      "\n",
      "Matthew 5:17    \"Think not that I have come to abolish the law and the \n",
      "prophets; I have come not to abolish them but to fulfil them. \n",
      "Matthew 5:18   For truly, I say to you, till heaven and earth pass away, \n",
      "not an iota, not a dot, will pass from the law until all is accomplished. \n",
      "Matthew 5:19   Whoever then relaxes one of the least of these commandments \n",
      "and teaches men so, shall be called least in the kingdom of heaven; but he \n",
      "who does them and teaches them shall be called great in the kingdom of \n",
      "heaven. \n",
      "Matthew 5:20   For I tell you, unless your righteousness exceeds that of \n",
      "the scribes and Pharisees, you will never enter the kingdom of heaven. \n",
      "\n",
      "There are several problems with this.  The most serious is that the\n",
      "Law was regarded by Jews at the time (and now) as binding on Jews, but\n",
      "not on Gentiles.  There are rules that were binding on all human\n",
      "beings (the so-called Noachic laws), but they are quite minimal.  The\n",
      "issue that the Church had to face after Jesus' death was what to do\n",
      "about Gentiles who wanted to follow Christ.  The decision not to\n",
      "impose the Law on them didn't say that the Law was abolished.  It\n",
      "simply acknowledged that fact that it didn't apply to Gentiles.  This\n",
      "is a simple answer, which I think just about everyone can agree to.\n",
      "(A discussion of the issue in more or less these terms is recorded\n",
      "in Acts 15.)\n",
      "\n",
      "However there's more involved.  In order to get a full picture of the\n",
      "role of the Law, we have to come to grips with Paul's apparent\n",
      "rejection of the Law, and how that relates to Jesus' commendation of\n",
      "the Law.  At least as I read Paul, he says that the Law serves a\n",
      "purpose that has been in a certain sense superceded.  Again, this\n",
      "issue isn't one of the abolition of the Law.  In the middle of his\n",
      "discussion, Paul notes that he might be understood this way, and\n",
      "assures us that that's not what he intends to say.  Rather, he sees\n",
      "the Law as primarily being present to convict people of their\n",
      "sinfulness.  But ultimately it's an impossible standard, and one that\n",
      "has been superceded by Christ.  Paul's comments are not the world's\n",
      "clearest here, and not everyone agrees with my reading.  But the\n",
      "interesting thing to notice is that even this radical position does\n",
      "not entail an abolition of the Law.  It still remains as an\n",
      "uncompromising standard, from which not an iota or dot may be removed.\n",
      "For its purpose of convicting of sin, it's important that it not be\n",
      "relaxed.  However for Christians, it's not the end -- ultimately we\n",
      "live in faith, not Law.\n",
      "\n",
      "While the theoretical categories they use are rather different, in the\n",
      "end I think Jesus and Paul come to a rather similar conclusion.  The\n",
      "quoted passage from Mat 5 should be taken in the context of the rest\n",
      "of the Sermon on the Mount, where Jesus shows us how he interprets the\n",
      "Law.  The \"not an iota or dot\" would suggest a rather literal reading,\n",
      "but in fact that's not Jesus' approach.  Jesus' interpretations\n",
      "emphasize the intent of the Law, and stay away from the ceremonial\n",
      "details.  Indeed he is well known for taking a rather free attitude\n",
      "towards the Sabbath and kosher laws.  Some scholars claim that Mat\n",
      "5:17-20 needs to be taken in the context of 1st Cent. Jewish\n",
      "discussions.  Jesus accuses his opponents of caring about giving a\n",
      "tenth of even the most minor herbs, but neglecting the things that\n",
      "really matter: justice, mercy and faith, and caring about how cups and\n",
      "plates are cleaned, but not about the fact that inside the people who\n",
      "use them are full of extortion and rapacity.  (Mat 23:23-25) This, and\n",
      "the discussion later in Mat 5, suggest that Jesus has a very specific\n",
      "view of the Law in mind, and that when he talks about maintaining the\n",
      "Law in its full strength, he is thinking of these aspects of it.\n",
      "Paul's conclusion is similar.  While he talks about the Law being\n",
      "superceded, all of the specific examples he gives involve the\n",
      "\"ceremonial law\", such as circumcision and the Sabbath.  He is quite\n",
      "concerned about maintaining moral standards.\n",
      "\n",
      "The net result of this is that when Paul talks about the Law being\n",
      "superceded, and Jesus talks about the Law being maintained, I believe\n",
      "they are talking about different aspects of the Law.  Paul is\n",
      "embroiled in arguments about circumcision.  As is natural in letters\n",
      "responding to specific situations, he's looking at the aspect of the\n",
      "Law that is currently causing trouble: the Law as specifically Jewish\n",
      "ceremonies.  He certainly does not intend to abolish divine standards\n",
      "of conduct.  On the other hand, when Jesus commends the Law, he seems\n",
      "to be talking the Law in its broadest implications for morals and\n",
      "human relationships, and deemphasizing those aspects that were later\n",
      "to give Paul so much trouble.\n",
      "\n",
      "It's unfortunate that people use the same terms in different ways, but\n",
      "we should be familiar with that from current conflicts.  Look at the\n",
      "way terms like \"family values\" take on special meaning from the\n",
      "current context.  Imagine some poor historian of the future trying to\n",
      "figure out why \"family values\" should be used as a code word for\n",
      "opposition to homosexuality in one specific period in the U.S.  I\n",
      "think Law had taken on a similar role in the arguments Paul was\n",
      "involved in.  Paul was clearly not rejecting all of the Jewish values\n",
      "that go along with the term \"Law\", any more than people who concerned\n",
      "about the \"family values\" movement are really opposed to family\n",
      "values.\n",
      "\n",
      "label is 1 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print(X_train_txt[0])\n",
    "print('label is', y_train[0], data_train.target_names[y_train[0]])\n",
    "print('-----------------------------------------')\n",
    "print(X_train_txt[3])\n",
    "print('label is', y_train[3], data_train.target_names[y_train[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "The document is currently a string, which scikit-learn can’t use directly. In order to feed predictive models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the `sklearn.feature_extraction.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(a)\n",
    "The class `DictVectorizer` can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\n",
    "\n",
    "`DictVectorizer` implements what is called one-of-K or “one-hot” coding for categorical. Recall that, in this method we replace each categorical attribute having m values with m binary attributes\n",
    "\n",
    "Refresh your knowledge about one-hot encoding and other possible discritisation methods in week 3 and extract the words (or tokens) in the text and count them by using `CountVectorizer` to build a dictionary which associates each word (token) in a text document with its frequency in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer()\n",
    "X_train = vectoriser.fit_transform(X_train_txt)\n",
    "X_test = vectoriser.transform(X_test_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(b)\n",
    "After “vectorizing” the data, what is the shape of X_train and X_test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(857, 18089) (570, 18089)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.(c)\n",
    "Are there any documents in X_test whose values are all 0? Why might this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test.sum(axis=1)==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is hypothetically possible - if every word in one of the test documents had never appeared in the training data. For long documents, this is exceedingly unlikely due to the appearance of grammatical \"words\" such as _the_, _is_, and so on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Now that we have \"vectorised\" our text, we want to choose the **best** attributes.\n",
    "\n",
    "Feature Selection in `scikit-learn` can be done using `SelectKBest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(a)\n",
    "Find out what the best 10 features were for your dataset, according to $\\chi^2$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atheism\n",
      "atheist\n",
      "atheists\n",
      "brian\n",
      "caltech\n",
      "christ\n",
      "islamic\n",
      "jesus\n",
      "keith\n",
      "ra\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "\n",
    "X_train_x2 = x2.fit_transform(X_train,y_train)\n",
    "X_test_x2 = x2.transform(X_test)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they correspond to your intuitions? Is there any evidence of the biases inherent in $\\chi^2$? What if you look at deeper than 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These seem like words that could be relevant to trying to distinguish between a religious discussion vs an atheist discuss.*\n",
    "\n",
    "*Perhaps surprising are words like _brian_, _keith_, and _caltech_, which are indicative of people who post to this forum (and perhaps not of the problem more generally). It's difficult to determine the rare/common distinction here, but it becomes a little clearer as we look further down the ranking.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.(b)\n",
    "Do the same thing for Mutual Information, instead of $\\chi^2$ (note that you want the classification version, not the regression version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570, 10) (857, 10)\n",
      "allan\n",
      "atheism\n",
      "atheists\n",
      "caltech\n",
      "cco\n",
      "it\n",
      "keith\n",
      "of\n",
      "schneider\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_mi = mi.fit_transform(X_train,y_train)\n",
    "X_test_mi = mi.transform(X_test)\n",
    "\n",
    "print(X_test_mi.shape, X_train_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we see more evidence of MI choosing frequently-occuring features, such as **it** and **of**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. \n",
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees.\n",
    "#### Exercise 4.(a) \n",
    "It’s likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer, but before the SelectKBest) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.(b)  \n",
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.(c) \n",
    "Try some different values for the cut-off for SelectKBest — is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between χ 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.8333333333333334\n",
      "x2 acc 0.8228070175438597\n",
      "mi acc 0.8070175438596491\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.8456140350877193\n",
      "x2 acc 0.8263157894736842\n",
      "mi acc 0.8140350877192982\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.5649122807017544\n",
      "x2 acc 0.5649122807017544\n",
      "mi acc 0.5649122807017544\n",
      "\n",
      " 1-nearest neighbour (with k= 1000 features):\n",
      "complete acc 0.6842105263157895\n",
      "x2 acc 0.6421052631578947\n",
      "mi acc 0.6228070175438597\n",
      "\n",
      " 5-nearest neighbour (with k= 1000 features):\n",
      "complete acc 0.656140350877193\n",
      "x2 acc 0.656140350877193\n",
      "mi acc 0.6017543859649123\n",
      "\n",
      " Decision Tree (with k= 1000 features):\n",
      "complete acc 0.8192982456140351\n",
      "x2 acc 0.7964912280701755\n",
      "mi acc 0.775438596491228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          KNeighborsClassifier(n_neighbors=1),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          '1-nearest neighbour',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "k = 1000\n",
    "\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train,y_train)\n",
    "X_train_x2 = x2.transform(X_train)\n",
    "X_test_x2 = x2.transform(X_test)\n",
    "\n",
    "\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train,y_train)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_test)\n",
    "\n",
    "\n",
    "Xs = [(X_train, X_test), (X_train_x2, X_test_x2), (X_train_mi, X_test_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_test_t.todense(), y_test)\n",
    "        print(X_name, 'acc',  acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
