{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 \n",
    "\n",
    "## Week 4 - workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be using scikit-learn to classify some data, and to evaluate some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Please load Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the gold–standard (“ground truth”) labels is called y ; create these variables for the car data.\n",
    "\n",
    "- **(a)** Load the data into a suitable format for scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('car.data', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        X.append(...) #all atts, excluding the class\n",
    "        y.append(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How many instances are there in this collection? How many attributes, and of what type(s)? What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print('There are', ..., 'instances')\n",
    "print('There are', ..., \"attributes, for example:\", ...)\n",
    "print('There are', .., \"class labels:\", ...)   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies: %s\" %str(label_counter.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "- **(a)** Write some functions that transform our **categorical** attributes into **numerical** attributes, by (perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "```python\n",
    "def convert_class(raw):\n",
    "    if raw==\"unacc\": return 0\n",
    "    elif raw==\"acc\": return 1\n",
    "    elif raw==\"good\": return 2\n",
    "    elif raw==\"vgood\": return 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could check this from the \"car.names\" file linked above\n",
    "# Here's one (somewhat inefficient) way of reading this from the data itself\n",
    "feature_1_values = set([X[i][0] for i in range(len(X))])\n",
    "feature_2_values = set([X[i][1] for i in range(len(X))])\n",
    "feature_3_values = set([X[i][2] for i in range(len(X))])\n",
    "feature_4_values = set([X[i][3] for i in range(len(X))])\n",
    "feature_5_values = set([X[i][4] for i in range(len(X))])\n",
    "feature_6_values = set([X[i][5] for i in range(len(X))])\n",
    "print(\"feature 1: %s\" %str(feature_1_values))\n",
    "print(\"feature 2: %s\" %str(feature_2_values))\n",
    "print(\"feature 3: %s\" %str(feature_3_values))\n",
    "print(\"feature 4: %s\" %str(feature_4_values))\n",
    "print(\"feature 5: %s\" %str(feature_5_values))\n",
    "print(\"feature 6: %s\" %str(feature_6_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_feature_1and2and6(raw):\n",
    "    if raw == \"low\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"high\": return 2\n",
    "    elif raw == \"vhigh\": return 3\n",
    "    # In general, we might want to catch unexpected values, too\n",
    "def convert_feature_3(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"3\": return 1\n",
    "    elif raw == \"4\": return 2\n",
    "    elif raw == \"5more\": return 3\n",
    "def convert_feature_4(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"4\": return 1\n",
    "    elif raw == \"more\": return 2\n",
    "def convert_feature_5(raw):\n",
    "    if raw == \"small\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"big\": return 2\n",
    "def convert_class(raw):\n",
    "    if raw == \"unacc\": return 0\n",
    "    elif raw == \"acc\": return 1\n",
    "    elif raw == \"good\": return 2\n",
    "    elif raw == \"vgood\": return 3\n",
    "\n",
    "X_ordinal = []\n",
    "for x in X:\n",
    "    f1, f2, f3, f4, f5, f6 = ...\n",
    "    f1 = ...\n",
    "    f2 = ...\n",
    "    f3 = ...\n",
    "    f4 = ...\n",
    "    f5 = ...\n",
    "    f6 = ...\n",
    "    x = [f1, f2, f3, f4, f5, f6]\n",
    "    X_ordinal.append(x)\n",
    "    \n",
    "#convert to int array to make sure everything is converted.\n",
    "X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "\n",
    "#convert ys\n",
    "y_numeric = []\n",
    "for this_y in y:\n",
    "    this_y = convert_class(this_y)\n",
    "    y_numeric.append(this_y)\n",
    "\n",
    "y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "\n",
    "print('X shape: {}, y shape: {}'.format(X_ordinal.shape, y_num.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Load the dataset again, this time as integers. Observe that we can actually build a model using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Read up on different implementations of the Naive Bayes classifier in `sklearn.naive_bayes`. Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Compare the accuracies of all three different kinds of Naive Bayes classifier. Does this accord with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "##print(dir(nb))\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(...)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(...)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(...)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** By default, this implementation of Naive Bayes uses Laplace smoothing. Turn this off, and see what happens — what is the significance of the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "\n",
    "# Gaussian NB doesn't use smoothing; all of the probabilities for the Gaussian are already non-zero\n",
    "# You can try this for yourself, but scikit-learn will flatly refuse to do it\n",
    "\n",
    "#mnb = MultinomialNB(alpha=0)\n",
    "\n",
    "#bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "mnb = MultinomialNB(alpha=...)\n",
    "bnb = BernoulliNB(alpha=...)\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** What happens if you increase the smoothing parameter instead? Calculate the accuracy for a range of values from 5 to 500. For the very large values, examine the predicted classes for the test instances — what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Let's not mess around, and go straight to a large value:\n",
    "mnb = MultinomialNB(alpha=...)\n",
    "bnb = BernoulliNB(alpha=...)\n",
    "\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a strategy does seem reasonable in light of the given values (such as *small, med, big*).\n",
    "A different strategy would be to `binarise` the attributes: to replace a categorical attribute having `m` values with `m binary attributes`. One way of doing this in scikit-learn is using the **OneHotEncoder** :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Check the shape of `X_trans` — how many attributes do we have now? Does this correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(...)\n",
    "X_trans = ohe.transform(...).toarray()\n",
    "\n",
    "print(X_trans.shape)\n",
    "print('X[0]:', X[0])\n",
    "print('X_trans[0]:', X_trans[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the dataset comprised of `one–hot attributes` into **train** and **test** sets. Compare the accuracies of the three Naive Bayes models using ordinal attributes with the three models using `one–hot attributes`: are you surprised? What can we infer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(...)\n",
    "    acc = gnb.score(...)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(...)\n",
    "    acc = mnb.score(...)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(...)\n",
    "    acc = bnb.score(...)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
